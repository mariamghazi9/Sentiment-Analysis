{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\n!pip install transformers\nnltk.download('wordnet')\nnltk.download('stopwords')\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport torch\nfrom torch import nn\nfrom transformers import BertModel\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\n#input path constant\ninput_path = \"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:20:41.463261Z","iopub.execute_input":"2022-02-05T19:20:41.46456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"markdown","source":"## 1. Read Data","metadata":{}},{"cell_type":"code","source":"def read_data(path):\n  df = pd.read_csv(path)\n  return df","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:34:14.546512Z","iopub.execute_input":"2022-02-05T18:34:14.546811Z","iopub.status.idle":"2022-02-05T18:34:14.551222Z","shell.execute_reply.started":"2022-02-05T18:34:14.546766Z","shell.execute_reply":"2022-02-05T18:34:14.550403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Split Data","metadata":{}},{"cell_type":"code","source":"def train_test_validate(x):\n  \n  X_train, X_test = train_test_split(x, test_size = 0.2, random_state=1, stratify=x[\"sentiment\"])\n\n  X_train, X_val = train_test_split(X_train, test_size = 0.1/0.8, random_state=1, stratify=X_train[\"sentiment\"])\n  return X_train, X_val, X_test","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:34:14.553075Z","iopub.execute_input":"2022-02-05T18:34:14.55385Z","iopub.status.idle":"2022-02-05T18:34:14.560196Z","shell.execute_reply.started":"2022-02-05T18:34:14.553814Z","shell.execute_reply":"2022-02-05T18:34:14.559402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Text pre-processing","metadata":{}},{"cell_type":"code","source":"def pre_process(review):\n\n  #remove punctuation\n  review = re.sub(r'[^\\w\\s]', ' ', review)\n\n  token = word_tokenize(review)\n\n  #lowercase\n  lowercase_words = []\n  for word in token:\n    word = word.lower()\n    lowercase_words.append(word)\n\n  #lemmatization\n  wml = WordNetLemmatizer()\n  lemma = []\n  for word in lowercase_words:\n      token = wml.lemmatize(word)\n      lemma.append(token)\n\n  #remove stop words\n  filtered_words = []\n  Stopwords = set(stopwords.words('english'))\n  for word in lemma:\n      if word not in Stopwords:\n          filtered_words.append(word)\n\n  return ' '.join(filtered_words)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:34:14.562847Z","iopub.execute_input":"2022-02-05T18:34:14.563338Z","iopub.status.idle":"2022-02-05T18:34:14.571357Z","shell.execute_reply.started":"2022-02-05T18:34:14.563301Z","shell.execute_reply":"2022-02-05T18:34:14.570487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"markdown","source":"## Dataset Class","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nlabels = {'negative':0,\n          'positive':1\n          }\n\nclass Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, df):\n\n        self.labels = [labels[label] for label in df[\"sentiment\"]]\n        self.texts = [tokenizer(pre_process(review) , \n                               padding='max_length', max_length = 512, truncation=True,\n                                return_tensors=\"pt\") for review in df[\"review\"]]\n\n    def classes(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_batch_labels(self, idx):\n        # Fetch a batch of labels\n        return np.array(self.labels[idx])\n\n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n\n        return batch_texts, batch_y\n","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:34:14.574032Z","iopub.execute_input":"2022-02-05T18:34:14.574571Z","iopub.status.idle":"2022-02-05T18:34:17.820152Z","shell.execute_reply.started":"2022-02-05T18:34:14.574519Z","shell.execute_reply":"2022-02-05T18:34:17.819432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    def __init__(self, dropout=0.1):\n        super(BertClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-cased')\n        self.dropout = nn.Dropout(p=dropout, inplace=False)\n        self.linear1 = nn.Linear(768, 512)\n        self.relu1 = nn.ReLU()\n        self.linear2 = nn.Linear(512, 256)\n        self.relu2 = nn.ReLU()\n        self.linear3 = nn.Linear(256, 128)\n        self.relu3 = nn.ReLU()\n        self.linear4 = nn.Linear(128, 64)\n        self.relu4 = nn.ReLU()\n        self.linear5 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n\n\n    def forward(self, input_id, mask):\n        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n        dropout_output = self.dropout(pooled_output)\n        linear1_output = self.linear1(dropout_output)\n        relu1 = self.relu1(linear1_output)\n        dropout_output = self.dropout(relu1)\n        linear2_output = self.linear2(dropout_output)\n        relu2 = self.relu2(linear2_output)\n        dropout_output = self.dropout(relu2)\n        linear3_output = self.linear3(dropout_output)\n        relu3 = self.relu3(linear3_output)\n        dropout_output = self.dropout(relu3)\n        linear4_output = self.linear4(dropout_output)\n        relu4 = self.relu4(linear4_output)\n        dropout_output = self.dropout(relu4)\n        linear5_output = self.linear5(dropout_output)\n\n        return linear5_output","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:34:17.822217Z","iopub.execute_input":"2022-02-05T18:34:17.823002Z","iopub.status.idle":"2022-02-05T18:34:17.833936Z","shell.execute_reply.started":"2022-02-05T18:34:17.82296Z","shell.execute_reply":"2022-02-05T18:34:17.833167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def binary_acc(y_pred, y_test): \n    y_pred_tag = torch.round(torch.sigmoid(y_pred)) \n    correct_results_sum = (y_pred_tag == y_test).sum().float() \n    acc = correct_results_sum\n    acc = torch.round(acc * 100) \n\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:34:17.836353Z","iopub.execute_input":"2022-02-05T18:34:17.836613Z","iopub.status.idle":"2022-02-05T18:34:17.847151Z","shell.execute_reply.started":"2022-02-05T18:34:17.836579Z","shell.execute_reply":"2022-02-05T18:34:17.846413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def train(model, train_data, val_data, learning_rate, epochs):\n\n    train, val = Dataset(train_data), Dataset(val_data)\n\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=1)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=1)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = Adam(model.parameters(), lr= learning_rate)\n    scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2)\n\n    if use_cuda:\n            model = model.cuda()\n            criterion = criterion.cuda()\n\n    min_loss = float('inf')\n    best_epoch = 0\n    results = []\n    for epoch_num in range(epochs):\n\n            total_acc_train = 0\n            total_loss_train = 0\n\n            for train_input, train_label in tqdm(train_dataloader): \n                train_label = train_label.to(device) \n                mask = train_input['attention_mask'].to(device) \n                input_id = train_input['input_ids'].squeeze(1).to(device) \n                optimizer.zero_grad() \n                output = model(input_id, mask) \n                 \n                train_label = train_label.to(torch.float32) \n                batch_loss = criterion(output, train_label.unsqueeze(1)) \n                total_loss_train += batch_loss.item() \n                 \n                acc = binary_acc(output,train_label.unsqueeze(1)) \n                total_acc_train += acc.item() \n  \n                batch_loss.backward() \n                optimizer.step() \n             \n             \n            total_acc_val = 0 \n            total_loss_val = 0 \n     \n            print(f'Acc: {total_acc_train/len(train_dataloader):.3f} ')\n            \n            \n            total_acc_val = 0\n            total_loss_val = 0\n\n            with torch.no_grad():\n\n                for val_input, val_label in val_dataloader:\n\n                    val_label = val_label.to(device)\n                    mask = val_input['attention_mask'].to(device)\n                    input_id = val_input['input_ids'].squeeze(1).to(device)\n\n                    output = model(input_id, mask)\n\n                    val_label = val_label.to(torch.float32)\n                    batch_loss = criterion(output, val_label.unsqueeze(1))\n                    total_loss_val += batch_loss.item()\n                    \n                    acc = binary_acc(output,val_label.unsqueeze(1)) \n                    total_acc_val += acc.item() \n            save_path = f'./model-epoch-{epoch_num}.pth'\n            torch.save(model.state_dict(), save_path)\n            if min_loss > total_loss_val:\n              min_loss = total_loss_val\n              best_epoch = epoch_num\n            scheduler.step(total_acc_val)\n            \n            print(\n                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n            print('Learning Rate: ', optimizer.param_groups[0]['lr'])\n            \n            results.append((epoch_num + 1,total_loss_train / len(train_data), \n                    total_acc_train / len(train_data), total_loss_val / len(val_data),\n                    total_acc_val / len(val_data)))\n    model.load_state_dict(torch.load(f'./model-epoch-{best_epoch}.pth'))\n    return results\n","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:40:24.733178Z","iopub.execute_input":"2022-02-05T18:40:24.733462Z","iopub.status.idle":"2022-02-05T18:40:24.750153Z","shell.execute_reply.started":"2022-02-05T18:40:24.733429Z","shell.execute_reply":"2022-02-05T18:40:24.749313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"def evaluate(model, test_data):\n\n    test = Dataset(test_data)\n\n    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n\n        model = model.cuda()\n\n    total_acc_test = 0\n    with torch.no_grad():\n\n        for test_input, test_label in test_dataloader:\n\n              test_label = test_label.to(device)\n              mask = test_input['attention_mask'].to(device)\n              input_id = test_input['input_ids'].squeeze(1).to(device)\n\n              output = model(input_id, mask)\n\n              acc = binary_acc(output,test_label.unsqueeze(1)) \n              total_acc_test += acc.item() \n    \n    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:34:17.869643Z","iopub.execute_input":"2022-02-05T18:34:17.870167Z","iopub.status.idle":"2022-02-05T18:34:17.880736Z","shell.execute_reply.started":"2022-02-05T18:34:17.87013Z","shell.execute_reply":"2022-02-05T18:34:17.879959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"x= read_data(input_path)\n\nX_train, X_val, X_test = train_test_validate(x);\n\nEPOCHS = 5\nmodel = BertClassifier()\nLR = 1e-5\n              \nresults = train(model, X_train, X_val, LR, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:40:32.252355Z","iopub.execute_input":"2022-02-05T18:40:32.253052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(model, X_test)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T18:40:12.71825Z","iopub.status.idle":"2022-02-05T18:40:12.71885Z","shell.execute_reply.started":"2022-02-05T18:40:12.71859Z","shell.execute_reply":"2022-02-05T18:40:12.718616Z"},"trusted":true},"execution_count":null,"outputs":[]}]}